---
title: "MCMC Simulation for Partial Order Inference"
author: "BayesianPO Package"
date: "`r Sys.Date()`"
output:
  #pdf_document:
  #  toc: true
  html_document: default
  #  toc: true
  #  toc_float: true
  #  code_folding: show
  #  theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

This tutorial demonstrates the Metropolis-Hastings Markov Chain Monte
Carlo (MH-MCMC) process applied to partial orders, implementing the
methodology described in [Nicholls et al.
(2024)](https://arxiv.org/abs/2212.05524).

The tutorial covers two main implementations:

1.  **Fixed Dimension MCMC**: Standard MCMC with fixed number of latent
    dimensions $K$
2.  **Reversible Jump MCMC**: Variable dimension sampling that
    automatically inference for $K$

The queue jumping noise model is implemented.

## Partial Order Model

The $K$-dimensional latent variable $U_{j,:}$ for the $j$-actor is

$$U_{j,:} \sim N(0, \Sigma_\rho), \text{ independent for each } j \in [1,...,M]$$
and determines a partial order $y$
$$y \sim p(\cdot|h(U))$$


## Queue Jump Error Model

Before the $i$-th entry $(y_i)$ is chosen: - There are $m - i + 1$
elements remaining $(y_{i:m})$ - With probability $p$: $y_i$ is chosen
randomly (ignoring order constraints) - With probability $(1-p)$: $y_i$
is compatible to the PO $h$.

The probability of a sequence $y_{1:m}$ given poset $h$ and noise
parameter $p$ is:

$$p(y_{1:m}|h, p) = \prod_{i=1}^{m-1} \frac{p}{m-j+1} + (1-p)\dfrac{|L_{y_i}[h(y_{1:m})]|}{|L[h(y_{1:m})]|}
 \,. $$

and the likelihood is $exp(-l(h,p; \{Y_i\}^N_{i=1} ))=\prod^N_{i=1} p(Y_i|h, p) $ where $Y_i=y_{i,1:m_i}$. 

### MCMC simulation when $K$ is fixed

\[ \pi(\rho|U) \propto \pi(U|\rho)\pi(\rho)\, ,\] \[\pi(p|h,\{Y_i\}^N_{i=1}) \propto \exp(-l(h,p;\{Y_i\}^N_{i=1})) \pi(p) \,,\]
\[ \pi(h(U)|p,\{Y_i\}^N_{i=1}) \propto \exp(-l(h,p;\{Y_i\}^N_{i=1}))\pi(U|\rho) \,. \]

### RJ-MCMC simulation when $K$ is variable

The reversible jump MCMC extends the standard algorithm by allowing the dimension $K$ to vary. The key moves are:

-   **Birth move**: $K \rightarrow K+1$ by adding a new latent dimension
-   **Death move**: $K \rightarrow K-1$ by removing a latent dimension

\[ \pi(K,h(U)|p,\{Y_i\}^N_{i=1}) \propto \exp(-l(h,p;\{Y_i\}^N_{i=1})) \pi(U|K,\rho) \pi(K) \]

The Jacobian term for the transformation is 1. 

## (1) Setup and Installation

```{r load-packages}

#source("../scripts/install_packages.R")
# Load the BayesianPO package
source("../R/utilities.R")
source("../R/mcmc.R")
source("../R/mcmc_rj.R")  # Load reversible jump MCMC
source("../R/analysis.R")

library(MASS)
library(mvtnorm)
library(ggplot2)
library(igraph)

```

## (2) Data Generation

The configuration

```{r}
n                <- 5          # number of items
N                <- 30         # number of total orders
K                <- ceiling(n/2)           # latent dimensions
min_sub          <- n          # min length of list
prob_noise_true  <- 0.1         # true queue-jump noise probability
rng_seed         <- 65          # reproducibility
noise_option <-  "queue_jump"
rho_true <- 0.9
```

## (3) Generate Synthetic Partial Order

```{r}

## 3.  Generate synthetic data -----------------------------------------------
synthetic_data <- generate_synthetic_data(
  n_items         = n,
  n_observations  = N,
  min_sub        = min_sub, 
  K               = K,
  rho_true        = rho_true,
  prob_noise_true = prob_noise_true,
  random_seed     = rng_seed
)

# Extract components
items <- synthetic_data$items
observed_orders <- synthetic_data$observed_orders
choice_sets <- synthetic_data$choice_sets
h_true <- synthetic_data$h_true
```

## (4) Visualize the True Partial Order

```{r plot-true-po, fig.width=12, fig.height=8}

plot_partial_order(
  h_true, items,
  title        = "True Partial Order",
  vertex_size   = 38,
  edge_arrow_size = 0.6,
  label_cex     = 1.3,  # even bigger text
  frame_width   = 2.3   # bolder outlines
)

```

## (5) Standard MCMC Implementation

```{r fixed-mcmc}
# MCMC parameters
fixed_cfg <- list(
  iters            = 30000,                      # total MH iterations
  dr               = 0.10,                      # rho proposal step
  rho_prior        = 1/6,                   # rho prior parameter
  noise_option     = "queue_jump",              # noise type
  noise_beta_prior = 9                        # queue-jump prob prior 
)

# Common arguments
args_fix <- list(
  observed_orders = observed_orders,
  choice_sets     = choice_sets,
  num_iterations  = fixed_cfg$iters,
  K               = K,
  dr              = fixed_cfg$dr,
  rho_prior       = fixed_cfg$rho_prior,
  noise_option    = "queue_jump",
  noise_beta_prior = fixed_cfg$noise_beta_prior,
  random_seed     = 1234
)

# 3.  Run the sampler
mcmc_fix <- do.call(mcmc_partial_order, args_fix)

```

## (6) Reversible Jump MCMC Implementation

The reversible jump MCMC allows the number of latent dimensions K to
vary during sampling, automatically determining the optimal model
complexity.

```{r rj-mcmc}
## ---- rj-mcmc -------------------------------------------------
# 1.  Configuration list ------------------------------------------------------
rj_cfg <- list(
  iters            = 30000, 
  dr               = 0.10,                      # rho step
  K_prior          = length(unique(unlist(choice_sets))),             
  noise_option     = "queue_jump",               
  noise_beta_prior = 9,                         # used **only** for queue-jump
  rho_prior        = 1/6
)

args_common <- list(
  observed_orders = observed_orders,
  choice_sets     = choice_sets,
  num_iterations  = rj_cfg$iters,
  dr              = rj_cfg$dr,
  rho_prior       = rj_cfg$rho_prior,
  K_prior         = rj_cfg$K_prior,
  noise_option    = "queue_jump",
  noise_beta_prior = rj_cfg$noise_beta_prior,
  random_seed     = 1234
)

# 2.  Run the sampler 
rj_results <- do.call(mcmc_partial_order_k, args_common)

```

## (7) Results and Diagnosics

```{r convergence, fig.width=12, fig.height=8}
# Plot log-likelihood traces
par(mfrow = c(1, 2))

# Check if we have valid data before plotting
plot(mcmc_fix$log_likelihood_trace, type = "l", 
       main = "Log-Likelihood (Fixed K)", xlab = "Iteration", ylab = "Log-Likelihood")

plot(rj_results$log_likelihood_trace, type = "l", 
       main = "Log-Likelihood (RJ-MCMC)", xlab = "Iteration", ylab = "Log-Likelihood")
```

We could select the burn-in period based on this.

```{r}
burn_in<-100
```

## (8) Parameter Estimation Comparison

```{r parameter-comparison}

# Fixed dimension estimates
rho_est_fixed <- mean(mcmc_fix$rho_trace[burn_in:length(mcmc_fix$rho_trace)])

# Reversible jump estimates  
rho_est_rj <- mean(rj_results$rho_trace[burn_in:length(rj_results$rho_trace)])
K_est_rj <- round(mean(rj_results$K_trace[burn_in:length(rj_results$K_trace)]))

# Create comparison table
comparison_df <- data.frame(
  Parameter = c("rho", "K"),
  True_Value = c(rho_true, K),
  Fixed_MCMC = c(rho_est_fixed,K),
  RJ_MCMC = c(rho_est_rj,  K_est_rj)
)

print(comparison_df)
```

## (9) Trace Plots

```{r trace-plots, fig.width=10, fig.height=8}
# Create comprehensive trace plots with prior/posterior comparisons
true_params <- list(
  rho_true = rho_true,
  prob_noise_true = prob_noise_true
)

config <- list(
  prior = list(
    rho_prior = fixed_cfg$rho_prior,
    noise_beta_prior =  fixed_cfg$noise_beta_prior
  ),
  noise = list(
    noise_option = "queue_jump"
  )
)

plot_mcmc_results(mcmc_fix, 
                 true_param = true_params,
                 config = config,
                 burn_in = 2)
```

## (10) RJ-MCMC Trace Plots

```{r rj-trace-plots, fig.width=12, fig.height=12}
# Create comprehensive trace plots for RJ-MCMC with K dimension
rj_config <- list(
  prior = list(
    rho_prior = rj_cfg$rho_prior,
    noise_beta_prior = rj_cfg$noise_beta_prior,
    K_prior = rj_cfg$K_prior
  ),
  noise = list(
    noise_option = "queue_jump"
  )
)

rj_true_params <- list(
  rho_true = rho_true,
  prob_noise_true =prob_noise_true,
  K_true = K
)

plot_mcmc_results(rj_results, 
                 true_param = rj_true_params,
                 config = rj_config,
                 burn_in = 2)
```

```{r}
mcmc_result_compare_parameter_analysis(
  mcmc_fixed = mcmc_fix,
  mcmc_rj = rj_results,
  param = "rho",
  burn_in = 0,
  true_value = true_params$rho_true
)

mcmc_result_compare_parameter_analysis(
  mcmc_fixed = mcmc_fix,
  mcmc_rj = rj_results,
  param = "prob_noise",
  burn_in = 0,
  true_value = true_params$prob_noise_true
)
```

## (11) Dimension Selection Analysis

```{r dimension-analysis, fig.width=10, fig.height=6}
# Analyze K posterior distribution
K_posterior <- table(rj_results$K_trace[burn_in:length(rj_results$K_trace)])
K_probs <- K_posterior / sum(K_posterior)

# Plot K posterior
par(mfrow = c(1, 1))

barplot(K_probs, main = "Posterior Distribution of K", 
        xlab = "Number of Dimensions (K)", ylab = "Posterior Probability",
        col = "lightblue", border = "darkblue")
abline(v = K + 0.5, col = "red", lty = 2, lwd = 2)

```


# Model Comparison and Validation

## (1) Posterior Predictive Checks

```{r posterior-predictive}
# Calculate burn-in index (convert to trace index)

threshold <- 0.5

# Extract post-burn-in partial order traces
h_trace_fixed_post_burnin <- mcmc_fix$h_trace[burn_in:length(rj_results$h_trace)]
h_trace_rj_post_burnin <- rj_results$h_trace[burn_in:length(rj_results$h_trace)]

# Compute posterior mean partial orders
cat("Computing posterior mean partial orders...\n")

# Fixed K MCMC: Average across all post-burn-in iterations
if (length(h_trace_fixed_post_burnin) > 0) {
  # Convert list of matrices to 3D array for easier averaging
  h_array_fixed <- array(0, dim = c(nrow(h_trace_fixed_post_burnin[[1]]), 
                                   ncol(h_trace_fixed_post_burnin[[1]]), 
                                   length(h_trace_fixed_post_burnin)))
  
  for (i in seq_along(h_trace_fixed_post_burnin)) {
    h_array_fixed[,,i] <- h_trace_fixed_post_burnin[[i]]
  }
  
  # Compute mean and apply threshold
  mean_h_fixed <- apply(h_array_fixed, c(1,2), mean)
  final_h_fixed <- (mean_h_fixed >= threshold) * 1
  
  # Apply transitive reduction to get minimal representation
  final_h_fixed <- transitive_reduction(final_h_fixed)
  
  cat("Fixed K MCMC: Averaged", length(h_trace_fixed_post_burnin), "post-burn-in samples\n")
} else {
  final_h_fixed <- matrix(0, nrow = nrow(h_true), ncol = ncol(h_true))
  cat("Warning: No post-burn-in samples for Fixed K MCMC\n")
}

# RJ-MCMC: Average across all post-burn-in iterations
if (length(h_trace_rj_post_burnin) > 0) {
  # Convert list of matrices to 3D array for easier averaging
  h_array_rj <- array(0, dim = c(nrow(h_trace_rj_post_burnin[[1]]), 
                                ncol(h_trace_rj_post_burnin[[1]]), 
                                length(h_trace_rj_post_burnin)))
  
  for (i in seq_along(h_trace_rj_post_burnin)) {
    h_array_rj[,,i] <- h_trace_rj_post_burnin[[i]]
  }
  
  # Compute mean and apply threshold
  mean_h_rj <- apply(h_array_rj, c(1,2), mean)
  final_h_rj <- (mean_h_rj >= threshold) * 1
  
  # Apply transitive reduction to get minimal representation
  final_h_rj <- transitive_reduction(final_h_rj)
  
  cat("RJ-MCMC: Averaged", length(h_trace_rj_post_burnin), "post-burn-in samples\n")
} else {
  final_h_rj <- matrix(0, nrow = nrow(h_true), ncol = ncol(h_true))
  cat("Warning: No post-burn-in samples for RJ-MCMC\n")
}

```

## (2) Visualize Estimated Partial Orders

```{r plot-estimates, fig.width=15, fig.height=5}
par(mfrow = c(1, 3))

plot_partial_order(
  h_true, items,
  title         = "True Partial Order",
  vertex_size   = 38,
  edge_arrow_size = 0.6,
  label_cex     = 1.3,  # even bigger text
  frame_width   = 2.3   # bolder outlines
)

plot_partial_order(
  final_h_fixed, items, title = "Fixed K MCMC Estimate\n(Posterior Mean)",
  vertex_size   = 38,
  edge_arrow_size = 0.6,
  label_cex     = 1.3,  # even bigger text
  frame_width   = 2.3   # bolder outlines
)


plot_partial_order(
  final_h_rj, items, title = "RJ-MCMC Estimate\n(Posterior Mean)",
  vertex_size   = 38,
  edge_arrow_size = 0.6,
  label_cex     = 1.3,  # even bigger text
  frame_width   = 2.3   # bolder outlines
)

```


## References

-   Nicholls, G. K., et al. (2024). Bayesian inference for partial
    orders. *arXiv preprint arXiv:2212.05524*.
-   Green, P. J. (1995). Reversible jump Markov chain Monte Carlo
    computation and Bayesian model determination. *Biometrika*, 82(4),
    711-732.

------------------------------------------------------------------------


